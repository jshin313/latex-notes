\documentclass{report}

\input{../preamble}
\input{../macros}
\input{../letterfonts}

\title{\Huge{Intro to Tensors Notes}\\}
\author{\huge{Jacob Shin}}
\date{}

\begin{document}

\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents
\pagebreak

% Start counter from 0
\setcounter{chapter}{-1}
\chapter{Tensor Definitions}
Based on the video series by \href{https://youtube.com/playlist?list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG}{eigenchris}

\dfn{Tensors as multidimensional arrays}{

Examples:
\begin{enumerate}
	\item Rank 0 Tensor: \textbf{Scalar}
		$$ [4] \text{ or } 4 $$ 
	\item Rank 1 Tensor: \textbf{Vector}
		$$
		\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} 
		$$ 

	\item Rank 2 Tensor: \textbf{2D-Matrix}
$$
		\begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix} 
$$ 
\item Rank 3 Tensor: \textbf{3D-Matrix} "Cube" of values

\end{enumerate}

Although tensors can be \emph{represented} as matrices/arrays, this definition doesn't describe what tensors \emph{actually} are since this definition doesn't explain the geometric meaning of tensors.
}

\dfn{Tensors as objects invariant under a change in coordinates}{
	\begin{enumerate}
		\item Tensors have \textbf{components} that change in a \textbf{predictable} way when the coordinates are changed
		\item Vectors are \textbf{invariant}, but vector components are not
		\item Example of something invariant under coordinate transformation: length
		\item Converting tensor components from one coordinate system to another is called a \textbf{Forward Transformation}, while doing the reverse if \textbf{Backwards Transformation}
	\end{enumerate}
}

\dfn{Tensors as a combination of vectors and convectors combined using the tensor product}{Best definition, but a bit abstract.}

\dfn{Tensors as partial derivatives and gradients that transform with the Jacobean matrix}{}

\chapter{Forward and Backwards Transformations}
Old Basis:
$$
\left\{ \vec{e_1}, \vec{e_2} \right\} 
$$ 
New Basis:
$$
\left\{ \tilde{\vec{e_1}}, \tilde{\vec{e_2}} \right\} 
$$ 

\section{Forward Transformation}
Convert from the old basis to the new basis:

$$ \tilde{\vec{e_1}} = c_1 \vec{e_1} + c_2 \vec{e_2} $$
$$ \tilde{\vec{e_2}} = c_3 \vec{e_1} + c_4 \vec{e_2} $$


where $c_1, c_2, c_3,\text{ and } c_4$ are scalar constants.

We can rewrite the above linear equations in matrix form by defining a \textbf{forward transformation matrix}, $F$

$$ F = \begin{bmatrix} c_1 & c_3 \\ c_2 & c_4 \end{bmatrix}  $$ 

$$ \begin{bmatrix} \tilde{\vec{e_1}} & \tilde{\vec{e_2}} \end{bmatrix} = \begin{bmatrix} \vec{e_1} & \vec{e_2} \end{bmatrix} F = \begin{bmatrix} \vec{e_1} & \vec{e_2} \end{bmatrix} \begin{bmatrix} c_1 & c_3 \\ c_2 & c_4 \end{bmatrix}  $$ 

Note that the $F$ matrix is flipped along the diagonal (transposed) from what we'd get if we multiplied the forward transformation matrix before the old basis.

$$ \tilde{F} = F^T = \begin{bmatrix} c_1 & c_2 \\ c_3 & c_4 \end{bmatrix}  $$ 
$$ \begin{bmatrix} \tilde{\vec{e_1}} \\ \tilde{\vec{e_2}} \end{bmatrix} = 	\tilde{F} \begin{bmatrix} \vec{e_1} \\ \vec{e_2} \end{bmatrix} = \begin{bmatrix} c_1 & c_2 \\ c_3 & c_4 \end{bmatrix}\begin{bmatrix} \vec{e_1} \\ \vec{e_2} \end{bmatrix}   $$ 
\section{Backwards Transformation}
Similarly, we can convert from the new basis to the old basis with a \textbf{backwards transformation}

$$ \vec{e_1} = a_1 \tilde{\vec{e_1}} + a_2 \tilde{\vec{e_2}} $$
$$ \vec{e_2} = a_3 \tilde{\vec{e_1}} + a_4 \tilde{\vec{e_2}} $$


where $a_1, a_2, a_3,\text{ and } a_4$ are scalar constants.

Rewrite in terms of the backwards transformation matrix, $B$
$$ B = \begin{bmatrix} a_1 & a_3 \\ a_2 & a_4 \end{bmatrix}  $$ 
$$ \begin{bmatrix} \vec{e_1} & \vec{e_2} \end{bmatrix} = \begin{bmatrix} \tilde{\vec{e_1}} & \tilde{\vec{e_2}} \end{bmatrix} B = \begin{bmatrix} \tilde{\vec{e_1}} & \tilde{\vec{e_2}} \end{bmatrix} \begin{bmatrix} a_1 & a_3 \\ a_2 & a_4 \end{bmatrix} $$ 

If we multiply $F$ and $B$ we get the identity matrix:
$$ BF = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}  $$ 

Thus one is the inverse of the other:
$$ B = F^{-1} $$ 

\section{Generalizing to N-Dimensions}
We can generalize to $n$ dimensions 

$$ F = \begin{bmatrix} F_{11} & F_{12} & \ldots & F_{1n} \\ F_{21} & F_{22} & \ldots & F_{2n} \\ \vdots & \vdots & \vdots & \vdots \\ F_{n1} & F_{n2} & \ldots & F_{nn} \end{bmatrix}$$ 
$$ \tilde{\vec{e_1}} = F_{11}\vec{e_1} + F_{21}\vec{e_2} + \ldots + F_{n1} \vec{e_n} $$ 
$$ \tilde{\vec{e_2}} = F_{12}\vec{e_1} + F_{22}\vec{e_2} + \ldots + F_{n2} \vec{e_n} $$ 
$$ \vdots $$ 
$$ \tilde{\vec{e_n}} = F_{1n} \vec{e_1} + F_{2n}\vec{e_2} + \ldots + F_{nn} \vec{e_n} $$ 

This can be written more simply as:
$$ \tilde{\vec{e_i}} = \sum_{j=1}^{n} F_{ji} \vec{e_j}$$ 

Similarly we have
$$ \vec{e_i} = \sum_{j=1}^{n} B_{ji} \tilde{\vec{e_j}}$$ 

We still have that $F$ and $B$ are inverses
$$ FB = \begin{bmatrix} 1 & 0 & 0 & \ldots \\ 0 & 1& 0 & \ldots \\ 0 & 0 & 1 &\ldots \\ \vdots \\ 0 & 0 & \ldots&1 \end{bmatrix}  $$ 

\begin{myproof}
$$ \vec{e_i} = \sum_{j=1}^{n} B_{ji} \tilde{\vec{e_j}}$$ 
$$ \vec{e_i} = \sum_{j=1}^{n} B_{ji} \left( \sum_{k=1}^{n} F_{kj} \vec{e_k} \right)= \sum_{j=1}^{n}  \left( \sum_{k=1}^{n} F_{kj} B_{ji}\vec{e_k} \right) =  \sum_{k=1}^{n}  \left( \sum_{j=1}^{n} F_{kj} B_{ji}\vec{e_k} \right)$$ 

Note that we want $\vec{e_1} = \vec{e_1}$, $\vec{e_2} = \vec{e_2}$, etc.
so that implies that
$$ 
\sum_j F_{kj}  B_{ji} = \delta_{ki} = 
\begin{cases}
	 0 & i \neq k \\
	 1 & i = k
\end{cases} 
$$ 

which means that the matrix $FB$ has $1$'s on the diagonals and $0$'s elsewhere (identity matrix).

\end{myproof}

\chapter{Vector Definition}
A vector is an example of a tensor.

\dfn{Vector Definition}
{
	\begin{enumerate}
		\item Naive definition: Array of numbers
			\begin{itemize}
				\item The list of numbers are the \emph{components} of the vector, not the vector itself. Remember that vectors are invariant under a coordinate transformation while the components are not.
			\end{itemize}
		\item Arrow in space
			\begin{itemize}
				\item A bit better than the above definition
				\item Not all vectors can be visualized as arrows (like functions!)
				\item An arrow is just a special type of vector: A \emph{Euclidean vector}
			\end{itemize}
		\item Member of a Vector Space
	\end{enumerate}

}
\dfn{Vector Space}{
	$$ \left(\underbrace{V}_{\text{Set of vectors}},\underbrace{S}_{\text{Set of Scalars}}, \underbrace{+}_{\text{Vector addition}}, \underbrace{\cdot}_{Vector scaling operator} \right) $$
	Vectors can be
	\begin{itemize}
		\item added together $+$
		\item Multiplied by a scalar $\cdot$
	\end{itemize}
}


\chapter{Vector Transformation Rules}
To convert the vector components from the old basis $\left\{ \vec{e_1}, \vec{e_2} \right\} $ to components in the new basis, $\left\{ \tilde{\vec{e_1}}, \tilde{\vec{e_2}} \right\} $, we apply the \emph{backwards} transformation on the old basis components:

Vector, $v$, in the old basis:
$$ \vec{v} = v_1 \vec{e_1} + v_2 \vec{e_2} = \begin{bmatrix} v_1 \\ v_2 \end{bmatrix}_{\vec{e_i}} $$ 
In the new basis
$$ \vec{v} = \tilde{v_1} \tilde{\vec{e_1}} + \tilde{v_2} \tilde{\vec{e_2}} = \begin{bmatrix} \tilde{v_1} \\ \tilde{v_2} \end{bmatrix}_{\tilde{\vec{e_i}}} $$ 
where $b_1, b_2, b_3 \text{ and }, b_4$ are scalars.

Converting vector components in the old basis to new
$$ B\begin{bmatrix} v_1 \\ v_2 \end{bmatrix}_{\vec{e_i}} = \begin{bmatrix} \tilde{v_1} \\ \tilde{v_2} \end{bmatrix}_{\tilde{\vec{e_i}}}   $$ 

When the basis vectors get larger, the "measuring stick" gets bigger, which means the components of the vector get smaller so that the length remains the same.

Similarly, when the basis vectors are rotated clockwise, from the perspective of the vector, the components have to be rotated counterclockwise to maintain the same orientation in space.

Since the vector components transform \emph{contrary} to the basis vectors, we say that the vector components are \emph{contravariant}.

\section{Generalization to $n$-dimensions}
\begin{myproof}
	$$ \vec{v} = \sum_{i=1}^{n} v_i \vec{e_i} = \sum_{j=1}^{n} \tilde{v_j}\tilde{\vec{e_j}}$$
	$$ \widetilde{\vec{e_j}} = \sum_{i=1}^{n} F_{ij} \vec{e_i} $$ 
	$$ \vec{e_j} = \sum_{i=1}^{n} B_{ij} \widetilde{\vec{e_i}} $$ 
	$$ \vec{v} =  \sum_{j=1}^{n} \tilde{v_j}\tilde{\vec{e_j}} =  \sum_{j=1}^{n} \tilde{v_j} \left( \sum_{i=1}^{n} F_{ij} \vec{e_i} \right) = \sum_{i=1}^{n} \left(\sum_{j=1}^{n} F_{ij}  \widetilde{v_j}\vec{e_i}\right) $$
	$$ \vec{v_i} = \sum_{j=1}^{n} F_{ij} \widetilde{\vec{v_j}} $$ 

This proves that converting the vector components in the new basis to the old basis requires the forward transformation
\end{myproof}

\section{Notation}
Since the vector components are contravariant (do the opposite of the basis vectors), we write the index for the vector components as a superscript:
$$ \vec{v} = \sum_{i=1}^{n} v^i \vec{e_i}  = \sum_{i=1}^{n} \widetilde{v^i} \widetilde{\vec{e_i}}$$ 

\chapter{What are covectors?}
Covectors are our second example of a tensor.

You can think of covectors as row vector. Row vectors are not simply just column vectors flipped on its side (only true in \emph{orthonormal basis}).

Covectors/Row vectors can both be thought of as functions that act on a vector:
$$ \alpha : V \to \R $$ 

Covectors exhibit linearity:
$$ \alpha\left( \vec{v} + \vec{w}\right) = \alpha(\vec{v}) + \alpha\left( \vec{w} \right)  $$ 
$$\alpha(n\vec{v}) = n \alpha(\vec{v})$$ 

Covectors are also elements of a vector space, called the dual vector space, $V^*$
$$ \left(n \alpha  \right)(\vec{v}) = n\alpha(\vec{v})  $$ 
$$ \left( \beta + \alpha \right) (\vec{v}) = \beta(\vec{v}) + \alpha(\vec{v})  $$ 

\end{document}
